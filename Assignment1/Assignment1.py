# -*- coding: utf-8 -*-
"""M24CSA029.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMZjRlNuo_hB5EF0bpIP-KtbtGaj0avL

Implemented Neural network from scratch incorporated different gradient descent algorithms such as Batch GD, Stochastic GD, Mini-batch GD. Plotted confusion matrix, accuracy and loss per epoch on different split ratio 70:30, 80:20, 90:10. Also used L2 Regularization techniques to prevent it from overfitting.Calculated total trainable and non-trainable parameters.

*   Dataset used : MNIST Dataset
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from tensorflow.keras.datasets import mnist

# Dataset Loading
(x, y), (x_test, y_test) = mnist.load_data()

# Combine dataset
x = np.vstack((x, x_test))
y = np.hstack((y, y_test))

# Normalization
x = x.reshape(x.shape[0], -1) / 255.0

# Random seed (Roll.no: M24CSA029)
SEED = 29
np.random.seed(SEED)

# One-hot encoding
def one_hot_encode(y, classes=10):
    one_hot = np.zeros((y.size, classes))
    one_hot[np.arange(y.size), y] = 1
    return one_hot

y = one_hot_encode(y)

# Train-test splitting
splits = [0.7, 0.8, 0.9]

def split_data(x, y, split_ratio):
    split_index = int(split_ratio * x.shape[0])
    indices = np.random.permutation(x.shape[0])
    return x[indices[:split_index]], y[indices[:split_index]], x[indices[split_index:]], y[indices[split_index:]]

# Parameters Defining
input_size = x.shape[1]
hidden1_size = 128
hidden2_size = 64
output_size = 10
batch_size = 24
epochs = 25
lambda_reg = 0.01

# Weight Initialization
def weight_initialize(layers):
    weights = {}
    for i in range(len(layers) - 1):
        weights[f"W{i + 1}"] = np.random.randn(layers[i], layers[i + 1]) * 0.01
        weights[f"b{i + 1}"] = np.ones((1, layers[i + 1]))
    return weights

# Trainable and Non-trainable parameters counting
def parameters_count(weights):
    trainable = sum(w.size for w in weights.values())
    non_trainable = 0
    return trainable, non_trainable

# Activation functions and their derivatives
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

# Loss function
def cross_entropy_loss(y_true, y_pred):
    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))

# Weight Update
def weight_update(weights, gradients, learning_rate, lambda_reg):
    for key in weights.keys():
        gradients[f"d{key}"] += lambda_reg * weights[key]
        weights[key] -= learning_rate * gradients[f"d{key}"]
    return weights

# Forward Propagation
def forward_propagation(x, weights):
    activation = {"A0": x}
    num_layers = len(weights) // 2

    for i in range(1, num_layers + 1):
        Z = np.dot(activation[f"A{i - 1}"], weights[f"W{i}"]) + weights[f"b{i}"]
        if i == num_layers:
            A = softmax(Z)
        else:
            A = relu(Z)
        activation[f"Z{i}"] = Z
        activation[f"A{i}"] = A

    return activation[f"A{num_layers}"], activation

# Backpropagation
def backward_propagation(x, y_true, activation, weights, lambda_reg):
    gradients = {}
    num_layers = len(weights) // 2
    m = x.shape[0]

    A_last = activation[f"A{num_layers}"]
    dZ = A_last - y_true

    for i in range(num_layers, 0, -1):
        gradients[f"dW{i}"] = (np.dot(activation[f"A{i - 1}"].T, dZ) / m) + (lambda_reg / m) * weights[f"W{i}"]
        gradients[f"db{i}"] = np.sum(dZ, axis=0, keepdims=True) / m

        if i > 1:
            dZ = np.dot(dZ, weights[f"W{i}"].T) * relu_derivative(activation[f"Z{i - 1}"])

    return gradients

# Neural Network from Scratch
def nn_scratch(x_train, y_train, x_test, y_test, layers, gd_type="mini-batch", batch_size=24, learning_rate=0.01, lambda_reg=0.01):
    weights = weight_initialize(layers)
    trainable_params, non_trainable_params = parameters_count(weights)
    print(f"Trainable parameters: {trainable_params}")
    print(f"Non-trainable parameters: {non_trainable_params}")
    history = {"train_loss": [], "train_accuracy": [], "test_loss": [], "test_accuracy": []}

    for epoch in range(epochs):
        indices = np.random.permutation(x_train.shape[0])
        x_train, y_train = x_train[indices], y_train[indices]

        # Batch GD
        if gd_type == "batch":
            y_pred, activation = forward_propagation(x_train, weights)
            gradients = backward_propagation(x_train, y_train, activation, weights, lambda_reg)
            weights = weight_update(weights, gradients, learning_rate, lambda_reg)

        # Stochastic GD
        elif gd_type == "sgd":
            for i in range(x_train.shape[0]):
                x_sample = x_train[i:i+1]
                y_sample = y_train[i:i+1]
                y_pred, activation = forward_propagation(x_sample, weights)
                gradients = backward_propagation(x_sample, y_sample, activation, weights, lambda_reg)
                weights = weight_update(weights, gradients, learning_rate, lambda_reg)

        # Mini-Batch GD
        elif gd_type == "mini-batch":
            for i in range(0, x_train.shape[0], batch_size):
                x_batch = x_train[i:i + batch_size]
                y_batch = y_train[i:i + batch_size]
                y_pred, activation = forward_propagation(x_batch, weights)
                gradients = backward_propagation(x_batch, y_batch, activation, weights, lambda_reg)
                weights = weight_update(weights, gradients, learning_rate, lambda_reg)

        # Training Metrics
        y_train_pred, _ = forward_propagation(x_train, weights)
        train_loss = cross_entropy_loss(y_train, y_train_pred) + (lambda_reg / (2 * x_train.shape[0])) * sum(
            np.sum(np.square(weights[f"W{i + 1}"])) for i in range(len(layers) - 1))
        train_accuracy = np.mean(np.argmax(y_train_pred, axis=1) == np.argmax(y_train, axis=1))

        # Test Metrics
        y_test_pred, _ = forward_propagation(x_test, weights)
        test_loss = cross_entropy_loss(y_test, y_test_pred) + (lambda_reg / (2 * x_test.shape[0])) * sum(
            np.sum(np.square(weights[f"W{i + 1}"])) for i in range(len(layers) - 1))
        test_accuracy = np.mean(np.argmax(y_test_pred, axis=1) == np.argmax(y_test, axis=1))

        # Store metrics
        history["train_loss"].append(train_loss)
        history["train_accuracy"].append(train_accuracy)
        history["test_loss"].append(test_loss)
        history["test_accuracy"].append(test_accuracy)

        print(
            f"Epoch {epoch + 1}/{epochs}, "
            f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, "
            f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}"
        )

    return weights, history

gd_types = ["batch", "sgd", "mini-batch"]
results = {}

for split in splits:
    x_train, y_train, x_test, y_test = split_data(x, y, split)
    print(f"\nTraining with Split Ratio: {split:.0%}")

    for gd_type in gd_types:
        print(f"Training with Gradient Descent Type: {gd_type.capitalize()}")

        layers = [input_size, hidden1_size, hidden2_size, output_size]
        trained_weights, history = nn_scratch(
            x_train, y_train, x_test, y_test, layers, gd_type=gd_type, batch_size=batch_size, learning_rate=0.01
        )
        results[(split, gd_type)] = (trained_weights, history)

        plt.figure(figsize=(12, 4))

        # Plot Accuracy
        plt.subplot(1, 2, 1)
        plt.plot(history["train_accuracy"], label="Train Accuracy")
        plt.plot(history["test_accuracy"], label="Test Accuracy")
        plt.title(f"Accuracy per Epoch (Split: {split:.0%}, GD Type: {gd_type.capitalize()})")
        plt.xlabel("Epoch")
        plt.ylabel("Accuracy")
        plt.legend()

        # Plot Loss
        plt.subplot(1, 2, 2)
        plt.plot(history["train_loss"], label="Train Loss")
        plt.plot(history["test_loss"], label="Test Loss")
        plt.title(f"Loss per Epoch (Split: {split:.0%}, GD Type: {gd_type.capitalize()})")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.show()

        # Confusion Matrix
        def evaluate_model(x, y, weights):
            y_pred, _ = forward_propagation(x, weights)
            y_pred_classes = np.argmax(y_pred, axis=1)
            y_true_classes = np.argmax(y, axis=1)
            cm = confusion_matrix(y_true_classes, y_pred_classes)
            accuracy = np.mean(y_true_classes == y_pred_classes)
            return cm, accuracy

        cm, accuracy = evaluate_model(x_test, y_test, trained_weights)
        print(f"Confusion Matrix (Split: {split:.0%}, GD Type: {gd_type.capitalize()}):")
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(10), yticklabels=range(10))
        plt.title(f"Confusion Matrix (Split: {split:.0%}, GD Type: {gd_type.capitalize()})")
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.show()

"""**Bonus Part**: Used weight initialization techniques such as Xavier and He initialization. Also used different activation functions such as sigmoid and tanh and plotted the loss and accuracy per epoch for different split ratio 70:30, 80:20, 90:10."""

# Weight Initialization: Xavier and He initialization techniques
def xavier_initialization(layers):
    weights = {}
    for i in range(len(layers) - 1):
        weights[f"W{i + 1}"] = np.random.randn(layers[i], layers[i + 1]) * np.sqrt(2 / (layers[i] + layers[i + 1]))
        weights[f"b{i + 1}"] = np.ones((1, layers[i + 1]))
    return weights

def he_initialization(layers):
    weights = {}
    for i in range(len(layers) - 1):
        weights[f"W{i + 1}"] = np.random.randn(layers[i], layers[i + 1]) * np.sqrt(2 / layers[i])
        weights[f"b{i + 1}"] = np.ones((1, layers[i + 1]))
    return weights

# Different activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    sig = sigmoid(x)
    return sig * (1 - sig)

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x) ** 2

# Forward Propagation with different activation function and weights initialization techniques
def forward_propagation_new(x, weights, activation_fn=relu):
    activation = {"A0": x}
    num_layers = len(weights) // 2

    for i in range(1, num_layers + 1):
        Z = np.dot(activation[f"A{i - 1}"], weights[f"W{i}"]) + weights[f"b{i}"]
        if i == num_layers:
            A = softmax(Z)
        else:
            if activation_fn == relu:
                A = relu(Z)
            elif activation_fn == sigmoid:
                A = sigmoid(Z)
            elif activation_fn == tanh:
                A = tanh(Z)
        activation[f"Z{i}"] = Z
        activation[f"A{i}"] = A

    return activation[f"A{num_layers}"], activation

# Backpropagation with different activation function and weights initialization techniques
def backward_propagation_new(x, y_true, activation, weights, lambda_reg, activation_fn_derivative=relu_derivative):
    gradients = {}
    num_layers = len(weights) // 2
    m = x.shape[0]

    A_last = activation[f"A{num_layers}"]
    dZ = A_last - y_true

    for i in range(num_layers, 0, -1):
        gradients[f"dW{i}"] = (np.dot(activation[f"A{i - 1}"].T, dZ) / m) + (lambda_reg / m) * weights[f"W{i}"]
        gradients[f"db{i}"] = np.sum(dZ, axis=0, keepdims=True) / m

        if i > 1:
            dZ = np.dot(dZ, weights[f"W{i}"].T) * activation_fn_derivative(activation[f"Z{i - 1}"])

    return gradients

# Mini-Batch Training
def nn_scratch_new(x_train, y_train, x_val, y_val, layers, activation_fn, activation_fn_derivative,
               batch_size=24, learning_rate=0.01, lambda_reg=0.01):

    if activation_fn == relu or activation_fn == relu_derivative:
        initialization = he_initialization
        init_name = "He"
    else:
        initialization = xavier_initialization
        init_name = "Xavier"

    # Initialize weights
    weights = initialization(layers)
    trainable_params, non_trainable_params = parameters_count(weights)
    print(f"Using Initialization: {init_name}")
    print(f"Trainable parameters: {trainable_params}")
    print(f"Non-trainable parameters: {non_trainable_params}")

    history = {"train_loss": [], "train_accuracy": [], "test_loss": [], "test_accuracy": []}

    for epoch in range(epochs):
        indices = np.random.permutation(x_train.shape[0])
        x_train, y_train = x_train[indices], y_train[indices]

        for i in range(0, x_train.shape[0], batch_size):
            x_batch = x_train[i:i + batch_size]
            y_batch = y_train[i:i + batch_size]
            y_pred, activation = forward_propagation_new(x_batch, weights, activation_fn)
            gradients = backward_propagation_new(x_batch, y_batch, activation, weights, lambda_reg, activation_fn_derivative)
            weights = weight_update(weights, gradients, learning_rate, lambda_reg)

        # Training Metrics
        y_train_pred, _ = forward_propagation_new(x_train, weights, activation_fn)
        train_loss = cross_entropy_loss(y_train, y_train_pred) + (lambda_reg / (2 * x_train.shape[0])) * sum(
            np.sum(np.square(weights[f"W{i + 1}"])) for i in range(len(layers) - 1))
        train_accuracy = np.mean(np.argmax(y_train_pred, axis=1) == np.argmax(y_train, axis=1))

        # Testing Metrics
        y_test_pred, _ = forward_propagation_new(x_test, weights, activation_fn)
        test_loss = cross_entropy_loss(y_test, y_test_pred) + (lambda_reg / (2 * x_test.shape[0])) * sum(
            np.sum(np.square(weights[f"W{i + 1}"])) for i in range(len(layers) - 1))
        test_accuracy = np.mean(np.argmax(y_test_pred, axis=1) == np.argmax(y_test, axis=1))

        # Store metrics
        history["train_loss"].append(train_loss)
        history["train_accuracy"].append(train_accuracy)
        history["test_loss"].append(test_loss)
        history["test_accuracy"].append(test_accuracy)

        print(
            f"Epoch {epoch + 1}/{epochs}, "
            f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, "
            f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}"
        )

    return weights, history


activation_functions = {
    "ReLU": (relu, relu_derivative),
    "Sigmoid": (sigmoid, sigmoid_derivative),
    "Tanh": (tanh, tanh_derivative)
}

for split in splits:
    x_train, y_train, x_val, y_val = split_data(x, y, split)
    print(f"\nTraining with Split Ratio: {split:.0%}")

    for act_name, (act_fn, act_derivative) in activation_functions.items():
        print(f"\nTraining with Activation Function: {act_name}")

        layers = [input_size, hidden1_size, hidden2_size, output_size]
        trained_weights_new, history_new = nn_scratch_new(
            x_train, y_train, x_val, y_val, layers, activation_fn=act_fn,
            activation_fn_derivative=act_derivative, batch_size=batch_size, learning_rate=0.01
        )

        plt.figure(figsize=(12, 4))

        # Plot Accuracy
        plt.subplot(1, 2, 1)
        plt.plot(history_new["train_accuracy"], label="Train Accuracy")
        plt.plot(history_new["test_accuracy"], label="Test Accuracy")
        plt.title(f"Accuracy per Epoch (Split: {split:.0%}, Act: {act_name})")
        plt.xlabel("Epoch")
        plt.ylabel("Accuracy")
        plt.legend()

        # Plot Loss
        plt.subplot(1, 2, 2)
        plt.plot(history_new["train_loss"], label="Train Loss")
        plt.plot(history_new["test_loss"], label="Test Loss")
        plt.title(f"Loss per Epoch (Split: {split:.0%}, Act: {act_name})")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.show()