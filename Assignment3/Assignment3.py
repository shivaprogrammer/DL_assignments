# -*- coding: utf-8 -*-
"""M24CSA020_M24CSA029_D24CSA005.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J8N9vadXg-zWFvLqlN-_2VDYE7ifs25D

**Assignment - 3 :**Building a Sequential Model for Sketch Generation from a given class or object name


**Submitted By:**


*   Pooja Naveen Poonia (M24CSA020)
*   Shivani Tiwari (M24CSA029)

*   Rinku Sadh (D24CSA005)
"""

import os
import requests

# List of 10 classes you want to download
classes = ["circle", "square", "triangle","sun","moon"]

BASE_URL = "https://storage.googleapis.com/quickdraw_dataset/full/simplified/{}.ndjson"

os.makedirs("quickdraw_data", exist_ok=True)

# Download each class
for category in classes:
    url = BASE_URL.format(category)
    response = requests.get(url)

    if response.status_code == 200:
        file_path = f"quickdraw_data/{category}.ndjson"
        with open(file_path, "wb") as f:
            f.write(response.content)
        print(f"Downloaded: {category}")
    else:
        print(f"Failed to download: {category}")

!pip install ndjson

import ndjson

# Load a single class example
with open("quickdraw_data/circle.ndjson", "r") as f:
    data = ndjson.load(f)

print(data[2].keys())

# Extract strokes from one sample
strokes = data[2]["drawing"]
print(strokes)

import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import ndjson
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, random_split

# Constants
EMBEDDING_DIM = 128
HIDDEN_DIM = 256
MAX_SEQ_LENGTH = 300
BATCH_SIZE = 32
LEARNING_RATE = 0.0001
NUM_EPOCHS = 10
TEACHER_FORCING_RATIO = 0.9

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# --- Dataset ---
class SketchDataset(Dataset):
    def __init__(self, classes, data_dir="quickdraw_data", max_seq_length=300):
        self.data = []
        self.labels = []
        self.class_to_idx = {cls: i for i, cls in enumerate(classes)}
        self.idx_to_class = {i: cls for cls, i in self.class_to_idx.items()}
        self.max_seq_length = max_seq_length

        for cls in classes:
            file_path = f"{data_dir}/{cls}.ndjson"
            if not os.path.exists(file_path):
                print(f"Warning: {cls}.ndjson not found, skipping.")
                continue

            with open(file_path, "r") as f:
                drawings = ndjson.load(f)

            for sample in drawings[:10000]:
                strokes = sample["drawing"]
                self.data.append(self.preprocess_strokes(strokes))
                self.labels.append(self.class_to_idx[cls])

    def preprocess_strokes(self, strokes):
        strokes_seq = []
        for stroke in strokes:
            x, y = np.array(stroke) / 255.0 - 0.5
            strokes_seq.extend(zip(x, y, [1] * len(x)))
        strokes_seq.append((0, 0, 0))
        strokes_seq = strokes_seq[:self.max_seq_length]
        while len(strokes_seq) < self.max_seq_length:
            strokes_seq.append((0, 0, 0))
        return torch.tensor(strokes_seq, dtype=torch.float32)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# --- Attention Mechanism ---
class Attention(nn.Module):
    def __init__(self, hidden_size):
        super(Attention, self).__init__()
        self.attn = nn.Linear(hidden_size * 2, hidden_size)
        self.v = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, hidden, encoder_outputs):
        seq_len = encoder_outputs.size(1)
        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)

        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
        attention_scores = self.v(energy).squeeze(2)
        attention_weights = torch.softmax(attention_scores, dim=1)

        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)
        return attention_weights, context_vector

# --- Encoder ---
class Encoder(nn.Module):
    def __init__(self, num_classes, embedding_dim, hidden_size):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(num_classes, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_size // 2, num_layers=2, batch_first=True)

    def forward(self, class_labels):
        x = self.embedding(class_labels).unsqueeze(1)
        _, (hidden, cell) = self.lstm(x)
        return hidden, cell

# --- Decoder ---
class Decoder(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Decoder, self).__init__()
        self.lstm = nn.LSTM(input_size + hidden_size // 2, hidden_size // 2, num_layers=2, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.attention = Attention(hidden_size // 2)

    def forward(self, x, hidden, cell, encoder_outputs):
        last_hidden = hidden[-1]
        attention_weights, context_vector = self.attention(last_hidden, encoder_outputs)

        x = torch.cat((x, context_vector.unsqueeze(1)), dim=2)
        output, (hidden, cell) = self.lstm(x, (hidden, cell))
        output = torch.cat((output, context_vector.unsqueeze(1)), dim=2)
        output = self.fc(output)

        return output, hidden, cell, attention_weights

# --- Full SketchRNN Model ---
class SketchRNN(nn.Module):
    def __init__(self, num_classes, embedding_dim, hidden_size, teacher_forcing_ratio=0.5):
        super(SketchRNN, self).__init__()
        self.encoder = Encoder(num_classes, embedding_dim, hidden_size)
        self.decoder = Decoder(3, hidden_size, 3)
        self.teacher_forcing_ratio = teacher_forcing_ratio

    def forward(self, class_labels, target_sketches=None, teacher_forcing_ratio=None):
        if teacher_forcing_ratio is None:
            teacher_forcing_ratio = self.teacher_forcing_ratio

        batch_size = class_labels.size(0)
        device = class_labels.device
        max_len = MAX_SEQ_LENGTH if target_sketches is None else target_sketches.size(1)

        hidden, cell = self.encoder(class_labels)
        encoder_outputs = hidden[-1].unsqueeze(1).repeat(1, max_len, 1)

        outputs = torch.zeros(batch_size, max_len, 3).to(device)
        decoder_input = torch.zeros(batch_size, 1, 3).to(device)

        for t in range(max_len):
            output, hidden, cell, _ = self.decoder(decoder_input, hidden, cell, encoder_outputs)
            outputs[:, t:t+1, :] = output

            teacher_force = random.random() < teacher_forcing_ratio
            if target_sketches is not None and teacher_force:
                decoder_input = target_sketches[:, t:t+1, :].detach()
            else:
                decoder_input = output.detach()

        return outputs

# --- Training Function ---
def train_model():
    classes = ["circle", "square", "triangle", "sun", "moon"]
    dataset = SketchDataset(classes)

    train_size = int(0.7 * len(dataset))
    val_size = int(0.15 * len(dataset))
    test_size = len(dataset) - train_size - val_size
    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SketchRNN(num_classes=len(classes), embedding_dim=EMBEDDING_DIM, hidden_size=HIDDEN_DIM).to(device)

    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    teacher_forcing_ratio = TEACHER_FORCING_RATIO

    for epoch in range(NUM_EPOCHS):
        model.train()
        total_loss = 0
        correct_predictions = 0
        total_samples = 0

        for sketches, labels in train_loader:
            sketches, labels = sketches.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(labels, target_sketches=sketches, teacher_forcing_ratio=teacher_forcing_ratio)

            # Compute loss
            loss = criterion(outputs, sketches)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            # Compute accuracy
            pred = torch.round(outputs)
            correct_predictions += (pred == sketches).sum().item()
            total_samples += sketches.numel()

        accuracy = (correct_predictions / total_samples) * 100

        teacher_forcing_ratio = max(0.5, teacher_forcing_ratio * 0.95)
        torch.cuda.empty_cache()

        print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%")

    return model, classes


if __name__ == "__main__":
    trained_model, class_names = train_model()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    random_idx = random.randint(0, len(class_names) - 1)

"""**Classes Visualization**


  

*   Sun Class






"""

import torch
import matplotlib.pyplot as plt
import random

# --- Step-by-Step Visualization ---
def visualize_strokes_step_by_step(model, class_idx, class_name):
    model.eval()
    with torch.no_grad():
        plt.figure(figsize=(5, 5))
        x, y = [0], [0]
        hidden, cell = model.encoder(class_idx)
        encoder_outputs = hidden[-1].unsqueeze(1).repeat(1, MAX_SEQ_LENGTH, 1)
        decoder_input = torch.zeros((1, 1, 3), device=class_idx.device)

        plt.ion()
        for _ in range(MAX_SEQ_LENGTH):
            output, hidden, cell, _ = model.decoder(decoder_input, hidden, cell, encoder_outputs)
            output = output.squeeze(1).cpu().numpy().flatten()

            x.append(x[-1] + output[0])
            y.append(y[-1] + output[1])

            plt.plot(x, y, 'bo-', markersize=2)
            plt.title(f"Generating: {class_name}")
            plt.pause(0.05)
            decoder_input = torch.tensor(output.reshape(1, 1, 3), dtype=torch.float32, device=class_idx.device)

        plt.ioff()
        plt.show()

random_idx = random.randint(0, len(class_names) - 1)
class_tensor = torch.tensor([random_idx], dtype=torch.long, device=device)

# Call visualization function
visualize_strokes_step_by_step(trained_model, class_tensor, "sun")

"""


*   Moon Class





"""

visualize_strokes_step_by_step(trained_model, class_tensor, "moon")

"""

*   Circle


"""

visualize_strokes_step_by_step(trained_model, class_tensor, "circle")

"""

*   Triangle


"""

visualize_strokes_step_by_step(trained_model, class_tensor, "triangle")

"""

*   Square


"""

visualize_strokes_step_by_step(trained_model, class_tensor, "square")